# -*- coding: utf-8 -*-
"""
need to create a 16 x 1 million (or so, or however users I have) matrix, 
where each user is represented with a length 16 numpy array of 1s and 0s, standing 
for whether they follow each of the social landmarks. Also add a 1 million long numpy 
array of Trues and Falses, standing for whether they are a 'new yorker' by our 
estimation, and finally another million-long numpy array of the UserIDs. Obviously, 
these all need to be ordered the same way.
"""
import os
import sys
import pandas as pd
import numpy as np
import glob
import datetime
# from binarysearch import binary_search

startTime = datetime.datetime.now()

def binary_search(L, v):
    lengthOfL = len(L)
    imin = 0
    imax = lengthOfL # imax always points to end of array (non inclusive).
    while imin < imax:
        # Computes midpoint for roughly equal partition.
        imid = int((imin + imax) / 2)
        if v == L[imid]:   # v found at index imid.
            return L[imid]
            break
        elif v < L[imid]:  # Changes imax index to search lower subarray.
            imax = imid
        else:              # Changes imin index to search upper subarray.
            imin = imid + 1
    
    if imin < imax:      # Found v
        # Handles repetitions: makes imid point to 1st greater than v.
        while imid < lengthOfL and v == L[imid]:
            imid += 1
        # Return userid
        # return L[imid]
        return True
    else:
        # if userid isn't in list, return -1
        # return -1
        return False

#Getting just the UserID column from my csv file that aggregates every csv of Search API
#results, generated by my readalltweets.py script. This might not be the way to go--
#might be better to take the code from that script and put it directly in here. 
allUserIDs = pd.read_csv('concat.csv', usecols = ['UserID'], error_bad_lines = False, 
    warn_bad_lines = True)
userLocations = pd.DataFrame(data={'UserID':[], 'location':[]})

#Going through the file for each social landmark that contains just the UserIDs for their
#followers, storing that list in a dataframe, which is stored within the list 
# 'sociallandmarkFollowers'
numberOfSocialLandmarks = 0
socialLandmarkFollowers = []
socialLandmarkNames = []
for csvfile in glob.glob('sociallandmarks/*followerdetails.csv'):
    try:
        csvUniv = open(csvfile, 'U')
        #This next part extracts just the name of the social landmark, so it strips 
        #the folder name and the ending ('followerids.csv')
        socialLandmarkHandle = csvfile[16:-19]
        print socialLandmarkHandle
        #storing the names of all of the social landmarks, to use in headers of the matrix
        socialLandmarkNames.append(socialLandmarkHandle)
        df = pd.read_csv(csvUniv, usecols = ['UserID'], error_bad_lines = False, 
        warn_bad_lines = True)
        dfloc = pd.read_csv(csvUniv, usecols = ['UserID', 'location'], engine='python', 
        warn_bad_lines = True)
        print 'created dataframes'
        #adding the social landmarks to a list of lists. The user ids for each 
        #social landmark is sorted so I can use binary search when creating
        #the matrix
        socialLandmarkFollowers.append([socialLandmarkHandle, sorted(df['UserID'].values)])
        print 'appended follower user ids to list'
        #adding the user ids from the social landmark list to my master list of
        #user ids (including the user ids pulled from the twitter Search API)
        allUserIDs = allUserIDs.append(df)
        print 'appended user ids'
        userLocations = userLocations.append(dfloc)
        print 'appended userids and locations'
        numberOfSocialLandmarks += 1
    except:
        print sys.exc_info()
        continue

# print 'sorting social landmark followers list of lists'
sociallandmarkFollowers = sorted(socialLandmarkFollowers, key=lambda x: x[0])

#dropping duplicates to speed up binary search and they're just not necessary
print 'dropping duplicates from all user ids list'
allUserIDs = allUserIDs.drop_duplicates(['UserID'])
userLocations = userLocations.drop_duplicates(['UserID'])
print allUserIDs.shape
print userLocations.shape

print 'converting alluserid dataframe to 1-D numpy array'
# converting to numpy 1-D array, for binary search.
allUserIDs = allUserIDs['UserID'].values

# Going through here is each userID we have from the search API AND
# social landmark , checking
# if UserID is in any of the sociallandmark lists, looping 

#Initializing the matrix here, filling with zeros so that it only needs
#to be edited if a user is found, 
numberOfUserIDs = len(allUserIDs)
matrix = np.zeros((numberOfUserIDs, numberOfSocialLandmarks))
sortedUserIDs = sorted(allUserIDs)

countofFoundUserIDs = 0


print "starting matrix loop"

# !!!need to wrap in determining if the user is a new yorker or not.
# for i in range(numberOfUserIDs):
for i in range(77):
    socialLandmarkCount = 0
    for socialLandmark in socialLandmarkFollowers:
        if binary_search(socialLandmark[1], sortedUserIDs[i]):
            # matrix[socialLandmarkCount,i] = 1
            matrix[i, socialLandmarkCount] = 1
            # print i, sortedUserIDs[i], socialLandmark[0]
            countofFoundUserIDs += 1
        socialLandmarkCount +=1
    socialLandmarkCount = 0

dfmatrix = pd.DataFrame(data=matrix, index=sortedUserIDs, columns=socialLandmarkNames)

dfmatrix.to_csv('mastermatrix.csv', index_label='UserID')

print dfmatrix.shape

endTime = datetime.datetime.now() - startTime

print "unique user ids:" 
print countofFoundUserIDs

print "runtime:"
print endTime

